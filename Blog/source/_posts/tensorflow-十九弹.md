---
title: tensorflow-十九弹
date: 2017-09-30 20:53:15
tags: tensorflow学习
categories: 学习
mathjax: true
---
话说在看论文的过程中看到了有很多神经网络的概念，不管什么学科都是建立在前人的基础之上的，所以了解一下现在相对来说比较古老的深度神经网络结构对我们充分理解深度学习，梳理学习过程是很有帮助的，因此这一讲我们着重了解一下各种其他类型的深度学习网络：

### 1.LeNet
必须要将他列为经典，必学的网络结构之一，我们现在神经网络中很多特点都是由他开始的，主要设计与应用的目的在于对手写数字进行识别，其网络结构如图：
<img src="http://blogimage-1251632003.cosgz.myqcloud.com/Lenet.png">

我们分析一下他的网络层：包括１．卷积；２．降采样；３．卷积；４．降采样；５权连接层；６．全连接层；不算输入与输出一共有６层，实际上现在看来LetNet网络的结构是十分简单的，但是他实现了一个卷积网络的所有结构．当然咯，实际上它只用了６种卷积核，和我们现在网络动则几十个卷积核比起来还是差得比较远，不过不要忘了，提出这个网络算法的时间是二十世纪，实际上忽略其深度上的差异，LeNet与我们现在所说的深度神经网络已经基本一致了．这个几乎是每一个想要学习深度学习的人入门所必学的吧.  
参考资料：
<sub>[1](http://blog.csdn.net/qiaofangjie/article/details/16826849)</sub>
### 2.AlexNet
AlexNet的网络结构为：   
<img src="http://blogimage-1251632003.cosgz.myqcloud.com/alexnet.png">(图片来自网络侵删)  
下面来详细分析一下过程，输入图像为224*224大小，采用的滤波核为11*11图上表示两台服务器，每一台服务器都提取48个特征，最终提取96个特征层，我们根据图示我们可以看到，给的stride为４，这个表示实际上间隔多少像素进行一次卷积操作，因此通过以上处理我们可以得到一个55*55大小的影像，这样我们得到的特征变为55*55*96,然后对计算结果进行ReLU映射，采用ReLU映射方式相比于sigmoid映射，能够在训练过程中更快的收敛，LRN函数进行局部区域归一化，局部区域归一化核大小为5*5，进行最大值池化，池化后影像大小变为27*27,这样得到一个27*27*96的输入影像进行第二次的卷积，这次采用的卷积核个数为256个，卷积过程为对每输入特征多个特征层相应区域进行卷积，然后求和，同样进行卷积ReLU映射，LRN处理，以及池化操作，处理后的结果作为第三层的输入，第三层使用384个13*13的新特征图与第二层操作方式相同，然而第三层没有进行池化操作，第四层依旧使用384个13*13的新特征图，与第三层的处理相同，没有进行池化操作，得到的结果作为第五层的输入，第五层采用256个13*13的新特征图，进行处理，为了防止过拟合，进行了池化操作，最后得到256个6*6的特征图，第六层和第七层是全连接层，采用4096个神经元进行全连接操作，最后输出一个100维的输出结果.  
从网络的结构来看可以看到，相比与LeNet网络AlexNet网络复杂了很多，通过多层特征网络的学习可以学习到各个目标物的特征，通过学习的特征进行全连接输出，实际上AlexNet在概念上有了一个特征学习的过程，对于未知输入不是直接对目标进行学习，而是对目标的特征进行学习，从而得到识别目标的特征．  
参考资料：
<sub>[1](http://www.cnblogs.com/gongxijun/p/6027747.html)</sub>
<sub>[2](http://blog.csdn.net/cyh_24/article/details/51440344)</sub>
<sub>[3](http://blog.csdn.net/sunbaigui/article/details/39938097)</sub>
<sub>[4](http://blog.csdn.net/liumaolincycle/article/details/50496499)</sub>
### 3.GoogleNet
相信通过上面的介绍大家对各种深度网络都有很深刻的理解了，实际上虽然网络结构各有各的差别，但是网络的基础结构是存在相似性的，其元处理包括卷积，非线性映射，归一化，降采样等操作，而经典的不同网络设计之间有些微妙的差别需要做过才有体会，GoogleNet在这里就不进行详细的介绍了，网络上的资料已经很详细了，给出一些参考资料大家可以深度研究一下：[参考资料](http://blog.csdn.net/lynnandwei/article/details/44458033)
