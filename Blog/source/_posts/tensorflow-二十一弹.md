---
title: tensorflow-二十一弹
date: 2017-12-17 11:37:45
tags: tensorflow学习
categories: 学习
---
今天我们讲讲DQN这个算法，这个著名的强化学习算法，在介绍DQN之前我们首先介绍一个Q-Learnning算法，一个强化学习的算法，这里有算法的详细介绍[传送门](http://blog.csdn.net/itplus/article/details/9361915)，如果大家仔细看完这篇文章相信会对Q-Learning 算法有一个比较清楚额认识了，其实算法整个来说比较简单，就是维护和更新一个Ｑ(state,action)的表，这个表中定义了每一个状态下采取一个动作的效用值，在学习到了这张表之后我们就可以对任意一个状态下的情况，从表中查找到其效用最大的action,依此类推．  
好了，以上问题算是一个很简单的问题了，因为其状态空间很小，6个房间，也就6x6的一个Ｑ表，其中还有一些元素为空值，因此可以用表的形式进行描述，而针对某些问题，比如我们马上要提到的玩FlappyBird游戏的问题，这个游戏状态空间是一个非常巨大的值，根本没有办法用一张表来描述，由此我们引出了今天的主题DQN算法，实际上对于维护一张状态空间巨大，甚至无限的情况来说，用Q表来描述肯定是不行的，为了能够进行描述，我们需要对Q表进行拟合，即用一个函数来对Q表进行近似，也就是说我们给一个输入，得到的是对应状态下所有动作的Q值，有了这个指导思想我们就可以稍微理解一下DQN了，实际上深度神经网络的过程就是用来构建Q值函数的过程，理解了一个之后我们再来看这篇文章[传送门](https://zhuanlan.zhihu.com/p/21421729)，如果大家能够有耐心看完这一篇文章相信大家对Q-Learning算法已经掌握的差不多了，如果大家没有耐心看也没关系，我们把文章内容总结起来就是通过不停的迭代获取Ｑ矩阵的过程，实际上我们会有一个效用函数也就是在当前状态下选择对应的reward的值，这个值加上原来的Ｑ值共同构成一个新Ｑ值，这样不停的对Ｑ进行迭代直到Ｑ值稳定，为什么Ｑ值会收敛，这个问题是可以经过证明的，证明过程比较复杂在这里就不进行讨论，有兴趣可以查看原文．  
相信大家看完以上两篇文章后对Ｑ-Learning算法已经有一定的认识了，而上文中我们提到，在很多时候我们面对的状态空间是巨大的，甚至可以说是无限巨大的，比如围棋游戏，19*19的棋盘，每一步都是任意的，这个Ｑ表过于巨大，因此无法用Ｑ表的形式进行表示，在这样的情况下我们就需要引入一个函数来描述这个Ｑ表，实际上是来描述这个Ｑ表的值的分布．好了到这里我们就知道Ｑ-Learning算法和深度学习算法是如何结合的了，实际上本质还是Ｑ-Learning，只是在学习过程中我们是通过函数来拟合Ｑ表，而这个拟合过程我们可以通过深度学习进行学习，关于DQN算法目前就谈到这里，这一次主要了解Ｑ-Learning算法的过程，而下一次我们会详细介绍深度学习算法和Ｑ-Learning算法的结合，并分析FlappyBird这个游戏的代码．
